# backend/app.py
import os
import google.generativeai as genai
import PyPDF2
import markdown
import time
import firebase_admin
from firebase_admin import credentials, firestore
from flask import Flask, request, jsonify, render_template
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pdf2image import convert_from_bytes
import pytesseract
from dotenv import load_dotenv
from flask_cors import CORS
import re

# --- LOAD .env ---
load_dotenv()

# --- CONFIG ---
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise ValueError("GEMINI_API_KEY missing in .env")

genai.configure(
    api_key=API_KEY,
    transport='rest'  # Add this
)
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# --- FIREBASE ---
cred = credentials.Certificate("../firebase-credentials.json")
firebase_admin.initialize_app(cred)
db = firestore.client()

app = Flask(__name__)
CORS(app)

def extract_text(pdf_stream):
    """OCR + Text Extract with logging"""
    print("  üîç Attempting text extraction...")
    text = ""
    
    try:
        reader = PyPDF2.PdfReader(pdf_stream)
        print(f"  üìÑ PDF has {len(reader.pages)} pages")
        
        for i, page in enumerate(reader.pages):
            page_text = page.extract_text() or ""
            text += page_text
            if i == 0:
                print(f"  ‚úì Page 1 extracted: {len(page_text)} chars")
        
        print(f"  ‚úÖ PyPDF2 extracted: {len(text)} chars total")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  PyPDF2 failed: {e}")
    
    if len(text.strip()) < 100:
        print("  üîÑ Text too short, trying OCR...")
        try:
            pdf_stream.seek(0)
            images = convert_from_bytes(pdf_stream.read())
            print(f"  üì∏ Converted to {len(images)} images")
            
            ocr_text = ""
            for i, img in enumerate(images):
                page_text = pytesseract.image_to_string(img)
                ocr_text += page_text
                if i == 0:
                    print(f"  ‚úì OCR page 1: {len(page_text)} chars")
            
            text = ocr_text
            print(f"  ‚úÖ OCR extracted: {len(text)} chars total")
        except Exception as e:
            print(f"  ‚ùå OCR failed: {e}")
    
    return text

def split_chunks(text):
    print("  ‚úÇÔ∏è  Splitting text into chunks...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    chunks = splitter.split_text(text)
    print(f"  ‚úÖ Created {len(chunks)} chunks")
    return chunks

def split_chunks(text):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    return splitter.split_text(text)

def generate_note(text):

    print("  ü§ñ Generating AI study note with gemini-pro-latest...")
    model = genai.GenerativeModel('models/gemini-pro-latest')

    prompt = f"""
    You are an expert universal study assistant. Your mission is to transform dense academic texts from **any language** into simplified, well-structured, and exceptionally easy-to-understand study notes.

    Your output must be in the **same language as the source text**. Follow these rules meticulously, as they are your core programming.

    **1. üí° The Golden Rule: Simplify Everything**
    *   This is your most important task. Your absolute priority is to make the content easy to understand for a student who finds the original text difficult.
    *   **Step A: Simplify Vocabulary.** Find any complex, technical, or **unfamiliar words** in the original text and replace them with simpler, more common words in that **same language**.
        *   *English Example:* Change "leverage synergistic paradigms" to "use teamwork effectively."
        *   *Spanish Example:* Change "implementar una metodolog√≠a vanguardista" to "usar un m√©todo nuevo y moderno."
    *   **Step B: Simplify Sentences.** Break down long, complex sentences into shorter, clearer ones that are easy to read and digest.
    *   **Step C: Simplify Structure.** Convert dense paragraphs into scannable bullet points, numbered lists, and short, focused sections.

    **2. ‚úçÔ∏è Annotation Rule: Translate ALL Simplified Words (NEW & IMPROVED)**
    *   Your annotation is not just for major keywords. For **every single word** that you simplified in Rule #1 because it was complex or unfamiliar, you **must** provide its Chinese translation.
    *   The format is always: `new simplified word (‰∏≠ÊñáÁøªËØë)`.
    *   **Example of Scope:** If the original text said "The *ubiquitous* nature of the *phenomenon*...", and you simplify it to "The *widespread* nature of the *event*...", your output must be: "The **widespread (ÊôÆÈÅçÁöÑ)** nature of the **event (‰∫ã‰ª∂)**..." This applies to all such words.

    **3. üé® Formatting Rule: Make it Engaging and Clear**
    *   **Headings and Emojis:** Use markdown headings (`#`, `##`) to create a clear structure. Add a relevant emoji next to each main heading to make it visually appealing (e.g., üîç for Definitions, ‚öôÔ∏è for Processes, ‚úÖ for Key Takeaways).
    *   **Highlighting:** Use **bold text** to emphasize the most important simplified keywords and concepts.
    *   **Tone:** Adopt a friendly, encouraging, and helpful tone, as if you are a tutor guiding the student through the material.

    **4. üéØ Content Rule: Be Comprehensive and Accurate**
    *   While simplifying, you must still cover **all major topics and key concepts** from the original text. Do not skip sections.
    *   Your notes should follow the same logical flow and structure as the source document.
    *   Extract only the most critical information‚Äîdefinitions, key arguments, and essential examples.

    **5. üß† Memory Aid: Add a Mnemonic Tip (ËÆ∞ÂøÜÊäÄÂ∑ß)**
    *   At the end of each major section, create a short, creative **Mnemonic Tip (ËÆ∞ÂøÜÊäÄÂ∑ß)**. This could be an acronym, a simple rhyme, or a memorable phrase to help the student recall the main points.

    **Constraint:**
    *   You must not add any new information that is not present in the original text. Your job is to simplify and structure, not to invent or add external knowledge.

    Here is the text to process:
    ---
    {text}
    ---
    """
    try:
        response = model.generate_content(prompt)
        return markdown.markdown(response.text)
    except Exception as e:
        print(f"  ‚ùå Generation failed: {e}")
        raise

def batch_save(collection, items, batch_size=100):
    batch = db.batch()
    for i, item in enumerate(items):
        if i % batch_size == 0 and i > 0:
            batch.commit()
            batch = db.batch()
        ref = collection.document()
        batch.set(ref, item)
    batch.commit()

# --- ROUTES ---
@app.route('/')
def home():
    return render_template('dashboard.html')

@app.route('/workspace/<project_id>')
def workspace(project_id):
    doc = db.collection('projects').document(project_id).get()
    name = doc.to_dict().get('name', 'Project') if doc.exists else 'Not Found'
    return render_template('workspace.html', project_id=project_id, project_name=name)

# --- API ---
@app.route('/get-projects')
def get_projects():
    docs = db.collection('projects').stream()
    return jsonify([{"id": d.id, "name": d.to_dict().get('name')} for d in docs])

@app.route('/create-project', methods=['POST'])
def create_project():
    name = request.json.get('name')
    ref = db.collection('projects').document()
    ref.set({'name': name, 'timestamp': firestore.SERVER_TIMESTAMP})
    return jsonify({"id": ref.id})

@app.route('/ask-chatbot/<project_id>', methods=['POST'])
def ask_chatbot(project_id):
    data = request.json
    q = data.get('question')
    src = data.get('source_id')
    
    # Get chunks
    all_chunks = []
    
    if src:
        # Get chunks from specific source
        chunk_docs = db.collection('projects').document(project_id) \
            .collection('sources').document(src) \
            .collection('chunks').stream()
        
        for doc in chunk_docs:
            all_chunks.extend(doc.to_dict().get('chunks', []))
    else:
        # Get chunks from all sources
        sources = db.collection('projects').document(project_id) \
            .collection('sources').stream()
        
        for source in sources:
            chunk_docs = source.reference.collection('chunks').stream()
            for doc in chunk_docs:
                all_chunks.extend(doc.to_dict().get('chunks', []))

    if not all_chunks:
        return jsonify({"answer": "Please upload a PDF first!"})

    # Use first 10 chunks as context
    context = "\n---\n".join(all_chunks[:10])
    
    model = genai.GenerativeModel('models/gemini-pro-latest')  # ‚úÖ Updated
    prompt = f"Answer using only this context:\n{context}\n\nQuestion: {q}\nAnswer:"
    
    try:
        answer = model.generate_content(prompt).text
        return jsonify({"answer": answer})
    except Exception as e:
        return jsonify({"answer": f"Error: {e}"})

@app.route('/generate-topic-note/<project_id>', methods=['POST'])
def topic_note(project_id):
    topic = request.json.get('topic')
    
    all_chunks = []
    sources = db.collection('projects').document(project_id).collection('sources').stream()
    
    for source in sources:
        chunk_docs = source.reference.collection('chunks').stream()
        for doc in chunk_docs:
            all_chunks.extend(doc.to_dict().get('chunks', []))
    
    context = "\n".join(all_chunks[:20])
    
    model = genai.GenerativeModel('models/gemini-pro-latest')  # ‚úÖ Updated
    prompt = f"Make a study note about: {topic}\nUse this:\n{context}"
    
    try:
        html = markdown.markdown(model.generate_content(prompt).text)
        return jsonify({"note_html": html})
    except Exception as e:
        return jsonify({"note_html": f"<p>Error generating note: {e}</p>"})

@app.route('/upload-source/<project_id>', methods=['POST'])
def upload_source(project_id):
    print("=" * 80)
    print(f"üìÅ UPLOAD REQUEST for project: {project_id}")
    print("=" * 80)
    
    # Check if files exist in request
    if 'pdfs' not in request.files:
        print("‚ùå ERROR: No 'pdfs' field in request!")
        print(f"Available fields: {list(request.files.keys())}")
        return jsonify({"error": "No files provided", "success": False}), 400
    
    files = request.files.getlist('pdfs')
    print(f"üì¶ Received {len(files)} file(s)")
    
    if not files or files[0].filename == '':
        print("‚ùå ERROR: Empty file list or no filename")
        return jsonify({"error": "No files selected", "success": False}), 400
    
    processed = []
    errors = []

    for idx, file in enumerate(files):
        print(f"\n{'‚îÄ' * 60}")
        print(f"üîÑ Processing file {idx + 1}/{len(files)}")
        print(f"{'‚îÄ' * 60}")
        
        filename = file.filename
        print(f"üìÑ Original filename: {filename}")
        
        safe_id = re.sub(r'[.#$/[\]]', '_', filename)
        print(f"üîê Safe ID: {safe_id}")

        try:
            # Step 1: Extract text
            print("üìñ Step 1: Extracting text from PDF...")
            file.stream.seek(0)
            text = extract_text(file.stream)
            
            print(f"‚úÖ Text extracted: {len(text)} characters")
            
            if not text.strip():
                error_msg = f"No text extracted from {filename}"
                print(f"‚ö†Ô∏è  WARNING: {error_msg}")
                errors.append({"filename": filename, "error": error_msg})
                continue
            
            print(f"üìù Preview: {text[:200]}...")

            # Step 2: Create source document
            print("üíæ Step 2: Creating source document in Firestore...")
            source_ref = db.collection('projects').document(project_id) \
                             .collection('sources').document(safe_id)
            
            source_ref.set({
                'filename': filename, 
                'timestamp': firestore.SERVER_TIMESTAMP,
                'character_count': len(text)
            })
            print(f"‚úÖ Source document created: projects/{project_id}/sources/{safe_id}")

            # Step 3: Generate AI note
            print("ü§ñ Step 3: Generating AI study note...")
            try:
                note_html = generate_note(text)
                print(f"‚úÖ Note generated: {len(note_html)} characters")
            except Exception as e:
                error_msg = f"AI generation failed: {str(e)}"
                print(f"‚ùå ERROR: {error_msg}")
                note_html = f"<p>AI note generation failed: {e}</p>"

            # Step 4: Save note in chunks
            print("üíæ Step 4: Saving note pages to Firestore...")
            chunk_size = 900000
            note_pages_saved = 0
            
            for i in range(0, len(note_html), chunk_size):
                chunk = note_html[i:i+chunk_size]
                page_num = i // chunk_size
                
                source_ref.collection('note_pages').document(f'page_{page_num}').set({
                    'html': chunk,
                    'order': page_num
                })
                note_pages_saved += 1
                print(f"  ‚úì Saved note page {page_num} ({len(chunk)} chars)")
            
            print(f"‚úÖ Total note pages saved: {note_pages_saved}")

            # Step 5: Split text for Q&A
            print("‚úÇÔ∏è  Step 5: Splitting text into chunks for Q&A...")
            text_chunks = split_chunks(text)
            print(f"‚úÖ Created {len(text_chunks)} text chunks")
            
            # Step 6: Save Q&A chunks
            print("üíæ Step 6: Saving Q&A chunks to Firestore...")
            chunks_docs_saved = 0
            
            for i in range(0, len(text_chunks), 100):
                batch_chunks = text_chunks[i:i+100]
                page_num = i // 100
                
                source_ref.collection('chunks').document(f'page_{page_num}').set({
                    'chunks': batch_chunks,
                    'order': page_num,
                    'count': len(batch_chunks)
                })
                chunks_docs_saved += 1
                print(f"  ‚úì Saved chunk page {page_num} ({len(batch_chunks)} chunks)")
            
            print(f"‚úÖ Total chunk pages saved: {chunks_docs_saved}")

            processed.append({
                "filename": filename, 
                "id": safe_id,
                "text_length": len(text),
                "note_pages": note_pages_saved,
                "chunk_pages": chunks_docs_saved
            })
            
            print(f"‚úÖ ‚úÖ ‚úÖ SUCCESS: {filename} fully processed!")

        except Exception as e:
            error_msg = f"Failed to process {filename}: {str(e)}"
            print(f"‚ùå CRITICAL ERROR: {error_msg}")
            import traceback
            print(traceback.format_exc())
            errors.append({"filename": filename, "error": error_msg})

    print("\n" + "=" * 80)
    print(f"üìä UPLOAD SUMMARY")
    print("=" * 80)
    print(f"‚úÖ Successfully processed: {len(processed)}")
    print(f"‚ùå Errors: {len(errors)}")
    
    if processed:
        print("\n‚úÖ Processed files:")
        for p in processed:
            print(f"  - {p['filename']} (ID: {p['id']})")
    
    if errors:
        print("\n‚ùå Errors:")
        for e in errors:
            print(f"  - {e['filename']}: {e['error']}")
    
    print("=" * 80)

    return jsonify({
        "success": len(processed) > 0,
        "processed": processed,
        "errors": errors
    })

@app.route('/test-models')
def test_models():
    try:
        print("üîç Checking available models...")
        models = genai.list_models()
        available = []
        for m in models:
            print(f"  Found: {m.name} - {m.display_name}")
            if 'generateContent' in m.supported_generation_methods:
                available.append({
                    'name': m.name,
                    'display_name': m.display_name,
                    'supported_methods': m.supported_generation_methods
                })
        return jsonify({
            "success": True,
            "available_models": available,
            "count": len(available)
        })
    except Exception as e:
        import traceback
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500

@app.route('/get-sources/<project_id>')
def get_sources(project_id):
    docs = db.collection('projects').document(project_id) \
             .collection('sources').stream()
    return jsonify([{
        "id": d.id,                 
        "filename": d.to_dict().get('filename')
    } for d in docs])

@app.route('/get-note/<project_id>/<path:source_id>')
def get_note(project_id, source_id):
    try:
        pages = db.collection('projects').document(project_id) \
                  .collection('sources').document(source_id) \
                  .collection('note_pages').stream()
        
        html = "".join(p.to_dict().get('html', '') for p in pages)
        return jsonify({"note_html": html or "<p>No note generated yet.</p>"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500



if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)